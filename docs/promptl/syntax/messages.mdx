---
title: Messages
description: Learn how to define messages in PromptL
---

## Overview

Messages are the core of LLM prompting. They define the conversation between the user and the assistant. Messages can have different roles, such as `system`, `user`, `assistant`, or `tool`. Each message role has a different meaning and purpose in the conversation.

## Message Tags

Plain text is automatically parsed by PromptL as a `system` message, although this can be changed in code.

To define other types of messages, you can use the following tags: `<system>`, `<user>`, `<assistant>`, and `<tool>`. In addition to this, you can also define messages with custom or dynamic roles by using the `<message>` tag.

```plaintext
<system>
  You are an expert content writer with deep knowledge of {{ industry }}.
  Always write in a clear, engaging style.
</system>

<user>
  Write a blog post about {{ topic }}. Use a {{ tone }} tone.
</user>

<assistant>
  Here's a draft blog post about {{ topic }}...
</assistant>

<message role='example'>
  Here's an example of high-quality content in this style...
</message>
```

### System Messages

System messages define instructions and provide general context to the assistant.

Although the this can also be used with user messages, it is recommended to use the `<system>` tag to define a clear separation between the two, where system messages have a higher authority.

```plaintext
<system>
  You are an expert content writer with deep knowledge of {{ industry }}.
  Always write in a clear, engaging style.
</system>
```

### User Messages

User messages can be used to include user input in the conversation. When including parameters straight from the user, it is recommended to use the `<user>` tag so that the assistant can understand the user's input, and it can prioritize instructions accordingly.

```plaintext
<user>
  Write a blog post about {{ topic }}. Use a {{ tone }} tone.
</user>
```

### Assistant Messages

LLMs always generate responses as assistant messages. These messages will be generated by the LLM, so you do not need to define them in your prompt, but sometimes it can be useful to fake a previous assistant response to guide the conversation. This way, when generating the response, the LLM will think it has already responded to the user, so it will follow the conversation flow.

```plaintext
<user>
  Hello, I am {{ name }}.
</user>

<assistant>
  Hi, {{ name }}! I am HistoryBot, your personal history assistant. What do you want to learn about today?
</assistant>

<user>
  {{ question }}
</user>
```

### Tool Messages

When defining tools in the configuration, the assistant may respond with tool requests. This is usually done when the assistant needs to execute a tool to gather more information or perform actions, which should be responded with the tool's output as a tool message. Read more about tools in your LLM's documentation.

Like with Assistant messages, this is not usually defined in the prompt, but it can be useful to fake previous tool responses to guide the conversation.

```plaintext
<user>
  What's the weather like in Barcelona?
</user>

<assistant>
  <tool-call id="123" name="get-weather" arguments={{ location: "Barcelona" }} />
</assistant>

<tool id="123">
  17ÂºC
</tool>
```
